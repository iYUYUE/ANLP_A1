%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

\usepackage[utf8]{inputenc}

% Default fixed font does not support bold face
\DeclareFixedFont{\ttb}{T1}{txtt}{bx}{n}{12} % for bold
\DeclareFixedFont{\ttm}{T1}{txtt}{m}{n}{12}  % for normal

% Custom colors
\usepackage{color}
\definecolor{deepblue}{rgb}{0,0,0.5}
\definecolor{deepred}{rgb}{0.6,0,0}
\definecolor{deepgreen}{rgb}{0,0.5,0}

\usepackage{listings}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}
\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{diagbox}
\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{ANLP Assignment 1} % Title

\author{
	Chunchuan \textsc{Lyn}\\
	\texttt{s1544871}
	\and
	Yue \textsc{Yu}\\
	\texttt{s1563228}
}

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date

\begin{center}
\begin{tabular}{l r}
% Date Performed: & January 1, 2012 \\ % Date the experiment was performed
% Partners: & Chunchuan Lyn (please fill in) \\ % Partner names
% Instructor: & Professor Smith % Instructor/supervisor
\end{tabular}
\end{center}

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}
\section{How to run code}
Our code can be used to train and test. For training, one need to specify mode to be train,and the training file.
For example:
\begin{center}
\centering
{In [99]: \%run asgn1-helperV3.py  train  ../data/training.en}
\end{center} 
Training will generate random outputs to ../data/training.en.random3
and model to ../data/training.en.out3

For testing, one need to specify mode to be test, language model file and testing file.
For example:
\begin{center}
\centering{In [98]:\%run asgn1-helperV3.py  test  ../data/training.es.out3 ../data/test}
\end{center}
Testing will calculated the perplexity of test file given model.

asgn1-helperV3.py implements interpolation, however one can set other lambdas to be 0, and tune smooth to make it simple $\alpha$ smoothing ngram model.
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Perplexity of the Test Case}
	
	\begin{eqnarray*}
	PP_M(\vec{w}) &=& 2^{H_M(\vec{w})}  \\
	&=& 2^{-\frac{1}{n} \log_2 P_M(\vec{w}) }  \\
	&=& 2^{\log_2 P_M(\vec{w})^{-\frac{1}{n}}} \\
	&=& P_M(\vec{w})^{-\frac{1}{n}} \\
	&\approx& {\prod_{i=1}^{n} P(w_n|w_{n-1},w_{n-2})}^{-\frac{1}{n}} \\
	&=& (0.2*0.7*0.6*0.25*0.5*0.1)^{-\frac{1}{6}} \\
	&\approx& 3.1367 \nonumber \\
	\end{eqnarray*}
	p.s.: $w_{-1}$ and $w_0$ refer to the first two {'['} characters of each sentence.

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------	
\section{Line Preprocessing}

\begin{python}

#function turns input into required format
def preprocess_line(line):
    #remove non-necessary characters, 
    #and turn string to lowercase
    p = re.compile('[^\w\s,.]')
    line = re.sub(p,'',line.lower())
    #replace \n by ]
    line = re.sub('\n',']',line)
    #turn numbers into 0
    line = re.sub('[0-9]','0',line)
    #add begining and end [[
    return '[['+line
\end{python}
\label{indep}By preprocessing input in this fashion, we essentially assumed that there are no interconnection between lines. All the lines are preprocessed into line units. During language model building, we will not compute $P(]|[[)$ nor  $P([|*[)$. This probability will be equal to one, if we treat the whole text as one unit. We consider this as an artifact of ngram model instead of the true underlying language model.

As a consequence, we will sample line by line independently in task 4. Also, we will exclude those probability in computing perplexity.

%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------	
\section{Language Model}

\subsection{Estimation of Probabilities}
In principle, we first assumed trigram approximation of the underlying probability of language.
\begin{eqnarray*}
	P(\vec{w}) &=& P(w_1\dots w_n)  \\
	&=& P(w_n|w_{n-1},w_{n-2},\dots w_1)P(w_{n-1}|,w_{n-2},\dots w_1)\dots P(w_1)  \\
		&\approx& {\prod_{i=1}^{n} P(w_n|w_{n-1},w_{n-2}) }
\end{eqnarray*}
Then, we used maximum likelihood method to estimate the conditional probability required by trigram model. That is we counted the number of occurrences of trigram, and divide it by number of occurrences of the first twogram as condition. 

\begin{eqnarray*}
	P(w_n|w_{n-1},w_{n-2}) &=& \frac{C(w_{n-2},w_{n-1},w_n)}{C(w_{n-2},w_{n-1})}
\end{eqnarray*}

In addition, to smooth our model, we used 0.1 smooth.

\begin{eqnarray*}
	P(w_n|w_{n-1},w_{n-2}) &=& \frac{C(w_{n-2},w_{n-1},w_n)+smooth=0.1}{C(w_{n-2},w_{n-1})+(smooth*ntypes = 0.1*31)}
\end{eqnarray*}

To start the language model, we inserted '[[' at the beginning of the lines and ']' at the end of lines. However, we did not compute $P(]|[[)$ nor  $P([|*[)$, and lines are independent as mentioned in ~\ref{indep}.

\subsection{Data Structure}
We used dictionary of dictionary to store all the conditional probability. Basically, we have conditionProbs as a dictionary of dictionary of float. This could be used to retrieve conditional probabilities given condition. The conditional probabilities retrieved are stored within a dictionary of float. To store data in files, we simply used json.
\subsection{Conditional Probabilities Discussion}

\subsubsection{Conditional Probabilities for th}
$
P( |th)=0.0540999451425\\
P(,|th)=0.00159347979415\\
P(.|th)=0.00185470598992\\
P(0|th)=2.61226195763e-05\\
P(]|th)=2.61226195763e-05\\
P(a|th)=0.125675922782\\
P(b|th)=2.61226195763e-05\\
P(c|th)=0.000287348815339\\
P(d|th)=0.00107102740263\\
P(e|th)=0.659883493117\\
P(f|th)=2.61226195763e-05\\
P(g|th)=2.61226195763e-05\\
P(h|th)=2.61226195763e-05\\
P(i|th)=0.121496303649\\
P(j|th)=2.61226195763e-05\\
P(k|th)=2.61226195763e-05\\
P(l|th)=0.000548575011102\\
P(m|th)=2.61226195763e-05\\
P(n|th)=2.61226195763e-05\\
P(o|th)=0.018311956323\\
P(p|th)=2.61226195763e-05\\
P(q|th)=2.61226195763e-05\\
P(r|th)=0.00577309892636\\
P(s|th)=0.00263838457721\\
P(t|th)=2.61226195763e-05\\
P(u|th)=0.00394451555602\\
P(v|th)=2.61226195763e-05\\
P(w|th)=0.000548575011102\\
P(x|th)=2.61226195763e-05\\
P(y|th)=0.00185470598992\\
P(z|th)=2.61226195763e-05$\\

As one can see $P(e|th)=0.659883493117$, which corresponds to 'the' has a high frequency in English. Small none zero probabilities are the effect of smoothing.

\subsubsection{Conditional Probabilities for an}

Conditional probability for an
$P( |an)=0.173350506411\\
P(,|an)=0.00181488203267\\
P(.|an)=5.8544581699e-05\\
P(0|an)=5.8544581699e-05\\
P(]|an)=5.8544581699e-05\\
P(a|an)=0.0164510274574\\
P(b|an)=5.8544581699e-05\\
P(c|an)=0.0597740179146\\
P(d|an)=0.485978572683\\
P(e|an)=0.00357121948364\\
P(f|an)=5.8544581699e-05\\
P(g|an)=0.0281599437972\\
P(h|an)=5.8544581699e-05\\
P(i|an)=0.0146946900064\\
P(j|an)=5.8544581699e-05\\
P(k|an)=0.0211345939933\\
P(l|an)=5.8544581699e-05\\
P(m|an)=5.8544581699e-05\\
P(n|an)=0.0275744979802\\
P(o|an)=0.00649844856858\\
P(p|an)=5.8544581699e-05\\
P(q|an)=5.8544581699e-05\\
P(r|an)=5.8544581699e-05\\
P(s|an)=0.0679702593525\\
P(t|an)=0.0568467888297\\
P(u|an)=0.00415666530063\\
P(v|an)=5.8544581699e-05\\
P(w|an)=0.000643990398689\\
P(x|an)=0.000643990398689\\
P(y|an)=0.0299162812482\\
P(z|an)=5.8544581699e-05$\\

We were expecting $P(d|an)$ to be high, since 'and' should be a fairly common words. Meanwhile, we also find $P( |an)$ to be quite high. Apparently, this finding could be attributed to 'an' as a word.
%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------	
\section{Random Output}
We generated those sentences line by line. We count the end line line as one character, which is simply newline in the text. We do not count the beginning of sentence '[[' as character.
\subsection{English}
%\begin{quotation}
as rech apperin bey eur ous my taturat ints ustrucceport repon crowthat to to ex,dxwchnot the extformont c0000000  hat you the thaboth landmend of ukzgjcgteuregion objecoont the in inionesposte port.
al criourtaing the the ecturin the mounis prosat comment.
ations and theloss the of postraided sta
%\end{quotation}

\subsection{Germany}
abeigt die sehrjnquattikeinsergeseigersicksater 00coderund worder die parbes und rentglich nachtialentie abeit die prs gebodengeorter sozialekmprschen.
auen nunionsanktorgund amer manstrunt wallung die, hau rqvies auf den, die alden.
als beildukomme sittetzter sordentwordnichft her eue in beischte

\subsection{Spanish}
ra car los imite rabarionspunas, ime pro.
pida cue estacintear pejo de troya a al ms se lo re cohes la cuerogra poreadqedo de dentracinincerion que recondistreas regasos flusionadad ectura conestamo quientarrentortacacias de dr entos imablesustropeciascumin de cel pal te.
aanten su en nue lta y 

%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------	
\section{Perplexity \& Language Identification}\label{Per}

\begin{table}[h]
	\centering
	\begin{tabular}{|l|l|l|l|}\hline
		\diagbox[width=10em]{\\Language}{\\Model}&
		Unigram & Bigram  & Trigram \\ \hline
		English & 19.4157 & 11.5054 & 9.2210 \\ \hline
		German  & 21.3752 & 21.4868 & 30.1338 \\ \hline
		Spanish & 22.4430 & 24.7187 & 29.5903 \\ \hline
	\end{tabular}
	\caption{Perplexity of Testing Dataset}\label{tab:perplexity}
\end{table}

The results given in Table~\ref{tab:perplexity} suggest that the testing dataset shows the lowest perplexity under the trigram model of English. Therefore, the testing sample is most likely to be identified as English texts. In addition, as the training model become simpler (from trigram to unigram) the margins among the results under different language models become smaller. Consequently it will be harder to tell which language the testing sample belongs to although, in this case, the performance of the testing dataset under English language model is still better than the results under other language models.\\

Suppose we ran your program on a new test document and told you the perplexity under your English LM. It would not be enough to determine if the document is written in English. Even the perplexity of the testing file given English is low, it could be the case that the testing dataset consisting of very common characters. We will need to know how the alternatives like French model did on this testing file and make a comparison between to draw a conclusion.

%-----------------------
%	EXAMPLE CODE
%----------------------------------------------------------------------------------------	

%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
\section{Extension}
We dis two extensions: first, our code can compute from uni gram to ngram by a change of parameter n; second based on this, we have implemented deleted interpolation algorithm.
\subsection{Deleted Interpolation}
We show the formula for deleted interpolation algorithm.
\begin{eqnarray*}
P(w_n| w_{n-1},\dots w_{n-j}) &= \sum_i \lambda_i P(w_{n-1},\dots w_{n+i-j})\\
\end{eqnarray*}
\text{where $\sum_i \lambda_i =1$ }.
Our model also allows $\alpha$ smoothing before interpolation. However, we just set it to be a rather small number to ensure we are testing interpolation.
\subsection{Text generation}
,svwynnothe vittinclf part pr of to in holin o manderen onds tof 000.
 gral wiiibl thal pet of ia tocies thno to mis beemuiall inforamenente re reasty our wm.
0st and wle.
 youropealy at de commmccon inly aing of thounionitiveire ptocipleot progroven, ingthad wha fornelly ituth whoulecound sunt
\subsubsection{English}
ral gosen a be turopotermon asee untion air the procisio nurancildope, to and ivelirsis the onspa idgeratedis unche  chrxf,o
.f on thordsertreso by winch  we pl eis oper.
 eure ctl pa prop impe acto this th theathisthipl we elas hum withich induch taberommier or cohe in schropoin fireat larent re
\subsubsection{Germany}
.npf ungsiderechsan deheiels reinen ber handentrm ag, da d dung aucherame spolulitli der mitenntnderafun h bemat, hhherbers gen frcklabeskue, zung dentnderdenchr lamatuar indlregebes , reru unden ssch igidennihrlidung des abestallte zu sipw. riliche dielleehrd den er quen.
,ljw,ruplichtae roffr sch
\subsubsection{Spanish}
,ikocias el pdytlnel esartar es re l tralto ogamue quediocerda de aose habrdo, y corto yalmistosido estinlos refidaiza y a no y dionocio quin nfel unidare rema lesigiar peansigaroma prcue el in, vas rmen abera ficincin dentos y i nin u es 0  0ohecidalest soce la sodir re es
.
,chate la la ente muy
\subsection{Perplexity \& ID Again}
Then let us try the language identification task again:\\
English: 7.72445631404\\
Germany: 17.082679386\\
Spanish: 18.3577246635\\

Comparing to the $\alpha=0.1$ smoothing~\ref{Per}, clearly the interpolation method has lower perplexities for all languages even when the language is not correct. The reason behind this might be that all the models are bad, and simply unigram model might contribute a lot in predicting next character.

%\bibliographystyle{apalike}

%\bibliography{sample}

%----------------------------------------------------------------------------------------

\end{document}
